;  IRL FlowCode Version: Version 10.0
;  c1995-2015: Visual Flow Coder by 2LResearch
;
;  File Name : eval.py_.vfc
;  File Date : 04:49:49 PM - 30:Apr:2023

event( from data import COCODetection, get_label_map, MEANS, COLORS );// 
event( from yolact import Yolact );// 
event( from utils.augmentations import BaseTransform, FastBaseTransform, Resize );// 
event( from utils.functions import MovingAverage, ProgressBar );// 
event( from layers.box_utils import jaccard, center_size, mask_iou );// 
event( from utils import timer );// 
event( from utils.functions import SavePath );// 
event( from layers.output_utils import postprocess, undo_image_transformation );// 
event( import pycocotools );// 
event( from data import cfg, set_cfg, set_dataset );// 
event( import numpy as np );// 
event( import torch );// 
event( import torch.backends.cudnn as cudnn );// 
event( from torch.autograd import Variable );// 
event( import argparse );// 
event( import time );// 
event( import random );// 
event( import cProfile );// 
event( import pickle );// 
event( import json );// 
event( import os );// 
event( from collections import defaultdict );// 
event( from pathlib import Path );// 
event( from collections import OrderedDict );// 
event( from PIL import Image );// 
event( import matplotlib.pyplot as plt );// 
event( import cv2 );// 
end();//

input( def str2bool(v):  );//
branch();//
path();//
path();// > --------------------------input 0 
branch( if v.lower() in ("yes", "true", "t", "y", "1"): );// 
path();//
set( return True );// 
path( elif v.lower() in ("no", "false", "f", "n", "0"): );// 
set( return False );// 
path( else: );// 
set( raise argparse.ArgumentTypeError("Boolean value expected.") );// 
bend( );//if v.lower() in ("yes", "true", "t", "y", "1"):
bend();//
end( );//def str2bool(v): > ----------------------- 0
end();//

input( def parse_args(argv=None):  );//
branch();//
path();//
path();// > --------------------------input 0 
set( parser = argparse.ArgumentParser(description="YOLACT COCO Evaluation") );// 
set( parser.add_argument( );// 
set( "--trained_model", );// 
set( default="weights/ssd300_mAP_77.43_v2.pth", );// 
set( type=str, );// 
set( help='Trained state_dict file path to open. If "interrupt", this will open the interrupt file.', );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--top_k", );// 
set( default=5, );// 
set( type=int, );// 
set( help="Further restrict the number of predictions to parse", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--cuda", default=True, type=str2bool, help="Use cuda to evaulate model" );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--fast_nms", );// 
set( default=True, );// 
set( type=str2bool, );// 
set( help="Whether to use a faster, but not entirely correct version of NMS.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--cross_class_nms", );// 
set( default=False, );// 
set( type=str2bool, );// 
set( help="Whether compute NMS cross-class or per-class.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--display_masks", );// 
set( default=True, );// 
set( type=str2bool, );// 
set( help="Whether or not to display masks over bounding boxes", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--display_bboxes", );// 
set( default=True, );// 
set( type=str2bool, );// 
set( help="Whether or not to display bboxes around masks", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--display_text", );// 
set( default=True, );// 
set( type=str2bool, );// 
set( help="Whether or not to display text (class [score])", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--display_scores", );// 
set( default=True, );// 
set( type=str2bool, );// 
set( help="Whether or not to display scores in addition to classes", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--display", );// 
set( dest="display", );// 
set( action="store_true", );// 
set( help="Display qualitative results instead of quantitative ones.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--shuffle", );// 
set( dest="shuffle", );// 
set( action="store_true", );// 
set( help="Shuffles the images when displaying them. Doesn't have much of an effect when display is off though.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--ap_data_file", );// 
set( default="results/ap_data.pkl", );// 
set( type=str, );// 
set( help="In quantitative mode, the file to save detections before calculating mAP.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--resume", );// 
set( dest="resume", );// 
set( action="store_true", );// 
set( help="If display not set, this resumes mAP calculations from the ap_data_file.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--max_images", );// 
set( default=-1, );// 
set( type=int, );// 
set( help="The maximum number of images from the dataset to consider. Use -1 for all.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--output_coco_json", );// 
set( dest="output_coco_json", );// 
set( action="store_true", );// 
set( help="If display is not set, instead of processing IoU values, this just dumps detections into the coco json file.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--bbox_det_file", );// 
set( default="results/bbox_detections.json", );// 
set( type=str, );// 
set( help="The output file for coco bbox results if --coco_results is set.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--mask_det_file", );// 
set( default="results/mask_detections.json", );// 
set( type=str, );// 
set( help="The output file for coco mask results if --coco_results is set.", );// 
set( ) );// 
set( parser.add_argument("--config", default=None, help="The config object to use.") );// 
set( parser.add_argument( );// 
set( "--output_web_json", );// 
set( dest="output_web_json", );// 
set( action="store_true", );// 
set( help="If display is not set, instead of processing IoU values, this dumps detections for usage with the detections viewer web thingy.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--web_det_path", );// 
set( default="web/dets/", );// 
set( type=str, );// 
set( help="If output_web_json is set, this is the path to dump detections into.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--no_bar", );// 
set( dest="no_bar", );// 
set( action="store_true", );// 
set( help="Do not output the status bar. This is useful for when piping to a file.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--display_lincomb", );// 
set( default=False, );// 
set( type=str2bool, );// 
set( help="If the config uses lincomb masks, output a visualization of how those masks are created.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--benchmark", );// 
set( default=False, );// 
set( dest="benchmark", );// 
set( action="store_true", );// 
set( help="Equivalent to running display mode but without displaying an image.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--no_sort", );// 
set( default=False, );// 
set( dest="no_sort", );// 
set( action="store_true", );// 
set( help="Do not sort images by hashed image ID.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--seed", );// 
set( default=None, );// 
set( type=int, );// 
set( help="The seed to pass into random.seed. Note: this is only really for the shuffle and does not (I think) affect cuda stuff.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--mask_proto_debug", );// 
set( default=False, );// 
set( dest="mask_proto_debug", );// 
set( action="store_true", );// 
set( help="Outputs stuff for scripts/compute_mask.py.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--no_crop", );// 
set( default=False, );// 
set( dest="crop", );// 
set( action="store_false", );// 
set( help="Do not crop output masks with the predicted bounding box.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--image", default=None, type=str, help="A path to an image to use for display." );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--images", );// 
set( default=None, );// 
set( type=str, );// 
set( help="An input folder of images and output folder to save detected images. Should be in the format input->output.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--video", );// 
set( default=None, );// 
set( type=str, );// 
set( help="A path to a video to evaluate on. Passing in a number will use that index webcam.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--video_multiframe", );// 
set( default=1, );// 
set( type=int, );// 
set( help="The number of frames to evaluate in parallel to make videos play at higher fps.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--score_threshold", );// 
set( default=0, );// 
set( type=float, );// 
set( help="Detections with a score under this threshold will not be considered. This currently only works in display mode.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--dataset", );// 
set( default=None, );// 
set( type=str, );// 
set( help="If specified, override the dataset specified in the config with this one (example: coco2017_dataset).", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--detect", );// 
set( default=False, );// 
set( dest="detect", );// 
set( action="store_true", );// 
set( help="Don't evauluate the mask branch at all and only do object detection. This only works for --display and --benchmark.", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--display_fps", );// 
set( default=False, );// 
set( dest="display_fps", );// 
set( action="store_true", );// 
set( help="When displaying / saving video, draw the FPS on the frame", );// 
set( ) );// 
set( parser.add_argument( );// 
set( "--emulate_playback", );// 
set( default=False, );// 
set( dest="emulate_playback", );// 
set( action="store_true", );// 
set( help="When saving a video, emulate the framerate that you'd get running in real-time mode.", );// 
set( ) );// 
set( parser.set_defaults( );// 
set( no_bar=False, );// 
set( display=False, );// 
set( resume=False, );// 
set( output_coco_json=False, );// 
set( output_web_json=False, );// 
set( shuffle=False, );// 
set( benchmark=False, );// 
set( no_sort=False, );// 
set( no_hash=False, );// 
set( mask_proto_debug=False, );// 
set( crop=True, );// 
set( detect=False, );// 
set( display_fps=False, );// 
set( emulate_playback=False, );// 
set( ) );// 
set( global args );// 
set( args = parser.parse_args(argv) );// 
branch( if args.output_web_json: );// 
path();//
set( args.output_coco_json = True );// 
bend( );//if args.output_web_json:
branch( if args.seed is not None: );// 
path();//
set( random.seed(args.seed) );// 
bend( );//if args.seed is not None:
bend();//
end( );//def parse_args(argv=None): > ----------------------- 0
set( iou_thresholds = [x / 100 for x in range(50, 100, 5)] );// 
set( coco_cats = {}   );//  Call prep_coco_cats to fill this
set( coco_cats_inv = {} );// 
set( color_cache = defaultdict(lambda: {}) );// 
end();//

input( def prep_display(  );//
branch();//
path();//
path();// > --------------------------input 0 
set( dets_out, );// 
set( img, );// 
set( h, );// 
set( w, );// 
set( undo_transform=True, );// 
set( class_color=False, );// 
set( mask_alpha=0.45, );// 
set( fps_str="", );// 
set( ): );// 
set( );//BEGIN MULTI LINE COMMENT
set( );//    Note: If undo_transform=False then im_h and im_w are allowed to be None.
set( );//END MULTI LINE COMMENT
branch( if undo_transform: );// 
path();//
set( img_numpy = undo_image_transformation(img, w, h) );// 
set( img_gpu = torch.Tensor(img_numpy).cuda() );// 
path( else: );// 
set( img_gpu = img / 255.0 );// 
set( h, w, _ = img.shape );// 
bend( );//if undo_transform:
branch( with timer.env("Postprocess"): );// 
path();//
set( save = cfg.rescore_bbox );// 
set( cfg.rescore_bbox = True );// 
set( t = postprocess( );// 
set( dets_out, );// 
set( w, );// 
set( h, );// 
set( visualize_lincomb=args.display_lincomb, );// 
set( crop_masks=args.crop, );// 
set( score_threshold=args.score_threshold, );// 
set( ) );// 
set( cfg.rescore_bbox = save );// 
bend( );//with timer.env("Postprocess"):
branch( with timer.env("Copy"): );// 
path();//
set( idx = t[1].argsort(0, descending=True)[: args.top_k] );// 
branch( if cfg.eval_mask_branch: );// 
path();//
set(  );//  Masks are drawn on the GPU, so don't copy
set( masks = t[3][idx] );// 
bend( );//if cfg.eval_mask_branch:
set( classes, scores, boxes = [x[idx].cpu().numpy() for x in t[:3]] );// 
bend( );//with timer.env("Copy"):
set( num_dets_to_consider = min(args.top_k, classes.shape[0]) );// 
loop( for j in range(num_dets_to_consider): );// 
branch( if scores[j] < args.score_threshold: );// 
path();//
set( num_dets_to_consider = j );// 
set( break );// 
bend( );//if scores[j] < args.score_threshold:
lend( );//for j in range(num_dets_to_consider):
set(  );//  Quick and dirty lambda for selecting the color for a particular index
set(  );//  Also keeps track of a per-gpu color cache for maximum speed
end();//
event( def get_color(j, on_gpu=None):  );//
branch();//
path();//
path();//  
set( global color_cache );// 
set( color_idx = (classes[j] * 5 if class_color else j * 5) % len(COLORS) );// 
branch( if on_gpu is not None and color_idx in color_cache[on_gpu]: );// 
path();//
set( return color_cache[on_gpu][color_idx] );// 
path( else: );// 
set( color = COLORS[color_idx] );// 
branch( if not undo_transform: );// 
path();//
set(  );//  The image might come in as RGB or BRG, depending
set( color = (color[2], color[1], color[0]) );// 
bend( );//if not undo_transform:
branch( if on_gpu is not None: );// 
path();//
set( color = torch.Tensor(color).to(on_gpu).float() / 255.0 );// 
set( color_cache[on_gpu][color_idx] = color );// 
bend( );//if on_gpu is not None:
set( return color );// 
bend( );//if on_gpu is not None and color_idx in color_cache[on_gpu]:
bend();//
end( );//def get_color(j, on_gpu=None): > ----------------------- 1
set(  );//  First, draw the masks on the GPU where we can do it really fast
set(  );//  Beware: very fast but possibly unintelligible mask-drawing code ahead
set(  );//  I wish I had access to OpenGL or Vulkan but alas, I guess Pytorch tensor operations will have to suffice
branch( if args.display_masks and cfg.eval_mask_branch and num_dets_to_consider > 0: );// 
path();//
set(  );//  After this, mask is of size [num_dets, h, w, 1]
set( masks = masks[:num_dets_to_consider, :, :, None] );// 
set(  );//  Prepare the RGB images for each mask given their color (size [num_dets, h, w, 1])
set( colors = torch.cat( );// 
set( [ );// 
set( get_color(j, on_gpu=img_gpu.device.index).view(1, 1, 1, 3) );// 
set( for j in range(num_dets_to_consider) );// 
set( ], );// 
set( dim=0, );// 
set( ) );// 
set( masks_color = masks.repeat(1, 1, 1, 3) * colors * mask_alpha );// 
set(  );//  This is 1 everywhere except for 1-mask_alpha where the mask is
set( inv_alph_masks = masks * (-mask_alpha) + 1 );// 
set(  );//  I did the math for this on pen and paper. This whole block should be equivalent to:
set(  );//     for j in range(num_dets_to_consider):
set(  );//         img_gpu = img_gpu * inv_alph_masks[j] + masks_color[j]
set( masks_color_summand = masks_color[0] );// 
branch( if num_dets_to_consider > 1: );// 
path();//
set( inv_alph_cumul = inv_alph_masks[: (num_dets_to_consider - 1)].cumprod(dim=0) );// 
set( masks_color_cumul = masks_color[1:] * inv_alph_cumul );// 
set( masks_color_summand += masks_color_cumul.sum(dim=0) );// 
bend( );//if num_dets_to_consider > 1:
set( img_gpu = img_gpu * inv_alph_masks.prod(dim=0) + masks_color_summand );// 
bend( );//if args.display_masks and cfg.eval_mask_branch and num_dets_to_consider > 0:
branch( if args.display_fps: );// 
path();//
set(  );//  Draw the box for the fps on the GPU
set( font_face = cv2.FONT_HERSHEY_DUPLEX );// 
set( font_scale = 0.6 );// 
set( font_thickness = 1 );// 
set( text_w, text_h = cv2.getTextSize( );// 
set( fps_str, font_face, font_scale, font_thickness );// 
set( )[0] );// 
set( img_gpu[0 : text_h + 8, 0 : text_w + 8] *= 0.6   );//  1 - Box alpha
bend( );//if args.display_fps:
set(  );//  Then draw the stuff that needs to be done on the cpu
set(  );//  Note, make sure this is a uint8 tensor or opencv will not anti alias text for whatever reason
set( img_numpy = (img_gpu * 255).byte().cpu().numpy() );// 
branch( if args.display_fps: );// 
path();//
set(  );//  Draw the text on the CPU
set( text_pt = (4, text_h + 2) );// 
set( text_color = [255, 255, 255] );// 
set( cv2.putText( );// 
set( img_numpy, );// 
set( fps_str, );// 
set( text_pt, );// 
set( font_face, );// 
set( font_scale, );// 
set( text_color, );// 
set( font_thickness, );// 
set( cv2.LINE_AA, );// 
set( ) );// 
bend( );//if args.display_fps:
branch( if num_dets_to_consider == 0: );// 
path();//
set( return img_numpy );// 
bend( );//if num_dets_to_consider == 0:
branch( if args.display_text or args.display_bboxes: );// 
path();//
loop( for j in reversed(range(num_dets_to_consider)): );// 
set( x1, y1, x2, y2 = boxes[j, :] );// 
set( color = get_color(j) );// 
set( score = scores[j] );// 
branch( if args.display_bboxes: );// 
path();//
set( cv2.rectangle(img_numpy, (x1, y1), (x2, y2), color, 1) );// 
bend( );//if args.display_bboxes:
branch( if args.display_text: );// 
path();//
set( _class = cfg.dataset.class_names[classes[j]] );// 
set( text_str = ( );// 
set( "%s: %.2f" % (_class, score) if args.display_scores else _class );// 
set( ) );// 
set( font_face = cv2.FONT_HERSHEY_DUPLEX );// 
set( font_scale = 0.6 );// 
set( font_thickness = 1 );// 
set( text_w, text_h = cv2.getTextSize( );// 
set( text_str, font_face, font_scale, font_thickness );// 
set( )[0] );// 
set( text_pt = (x1, y1 - 3) );// 
set( text_color = [255, 255, 255] );// 
set( cv2.rectangle( );// 
set( img_numpy, (x1, y1), (x1 + text_w, y1 - text_h - 4), color, -1 );// 
set( ) );// 
set( cv2.putText( );// 
set( img_numpy, );// 
set( text_str, );// 
set( text_pt, );// 
set( font_face, );// 
set( font_scale, );// 
set( text_color, );// 
set( font_thickness, );// 
set( cv2.LINE_AA, );// 
set( ) );// 
bend( );//if args.display_text:
lend( );//for j in reversed(range(num_dets_to_consider)):
bend( );//if args.display_text or args.display_bboxes:
set( return img_numpy );// 
bend();//
end( );//def prep_display( > ----------------------- 0
end();//

input( def prep_benchmark(dets_out, h, w):  );//
branch();//
path();//
path();// > --------------------------input 0 
branch( with timer.env("Postprocess"): );// 
path();//
set( t = postprocess( );// 
set( dets_out, w, h, crop_masks=args.crop, score_threshold=args.score_threshold );// 
set( ) );// 
bend( );//with timer.env("Postprocess"):
branch( with timer.env("Copy"): );// 
path();//
set( classes, scores, boxes, masks = [x[: args.top_k] for x in t] );// 
branch( if isinstance(scores, list): );// 
path();//
set( box_scores = scores[0].cpu().numpy() );// 
set( mask_scores = scores[1].cpu().numpy() );// 
path( else: );// 
set( scores = scores.cpu().numpy() );// 
bend( );//if isinstance(scores, list):
set( classes = classes.cpu().numpy() );// 
set( boxes = boxes.cpu().numpy() );// 
set( masks = masks.cpu().numpy() );// 
bend( );//with timer.env("Copy"):
branch( with timer.env("Sync"): );// 
path();//
set(  );//  Just in case
set( torch.cuda.synchronize() );// 
bend( );//with timer.env("Sync"):
bend();//
end( );//def prep_benchmark(dets_out, h, w): > ----------------------- 0
end();//

input( def prep_coco_cats():  );//
branch();//
path();//
path();// > --------------------------input 0 
set(  );// Prepare inverted table for category id lookup given a coco cats object.***0
loop( for coco_cat_id, transformed_cat_id_p1 in get_label_map().items(): );// 
set( transformed_cat_id = transformed_cat_id_p1 - 1 );// 
set( coco_cats[transformed_cat_id] = coco_cat_id );// 
set( coco_cats_inv[coco_cat_id] = transformed_cat_id );// 
lend( );//for coco_cat_id, transformed_cat_id_p1 in get_label_map().items():
bend();//
end( );//def prep_coco_cats(): > ----------------------- 0
end();//

input( def get_coco_cat(transformed_cat_id):  );//
branch();//
path();//
path();// > --------------------------input 0 
set(  );// transformed_cat_id is [0,80) as indices in cfg.dataset.class_names***0
set( return coco_cats[transformed_cat_id] );// 
bend();//
end( );//def get_coco_cat(transformed_cat_id): > ----------------------- 0
end();//

input( def get_transformed_cat(coco_cat_id):  );//
branch();//
path();//
path();// > --------------------------input 0 
set(  );// transformed_cat_id is [0,80) as indices in cfg.dataset.class_names***0
set( return coco_cats_inv[coco_cat_id] );// 
bend();//
end( );//def get_transformed_cat(coco_cat_id): > ----------------------- 0
end();//;

input( class Detections:  );//
branch();//
path();//
path();//  
end();//
event( def __init__(self):  );//
branch();//
path();//
path();//  
set( self.bbox_data = [] );// 
set( self.mask_data = [] );// 
bend();//
end( );//def __init__(self): > ----------------------- 1
end();//
event( def add_bbox(self, image_id: int, category_id: int, bbox: list, score: float):  );//
branch();//
path();//
path();//  
set(  );// Note that bbox should be a list or tuple of (x1, y1, x2, y2)***1
set( bbox = [bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1]] );// 
set(  );//  Round to the nearest 10th to avoid huge file sizes, as COCO suggests
set( bbox = [round(float(x) * 10) / 10 for x in bbox] );// 
set( self.bbox_data.append( );// 
set( { );// 
set( "image_id": int(image_id), );// 
set( "category_id": get_coco_cat(int(category_id)), );// 
set( "bbox": bbox, );// 
set( "score": float(score), );// 
set( } );// 
set( ) );// 
bend();//
end( );//def add_bbox(self, image_id: int, category_id: int, bbox: list, score: float): > ----------------------- 1
end();//
event( def add_mask(  );//
branch();//
path();//
path();//  
set( self, image_id: int, category_id: int, segmentation: np.ndarray, score: float );// 
set( ): );// 
set(  );// The segmentation should be the full mask, the size of the image and with size [h, w].***2
set( rle = pycocotools.mask.encode(np.asfortranarray(segmentation.astype(np.uint8))) );// 
set( rle["counts"] = rle["counts"].decode( );// 
set( "ascii" );//
set( )   );//  json.dump doesn't like bytes strings
set( self.mask_data.append( );// 
set( { );// 
set( "image_id": int(image_id), );// 
set( "category_id": get_coco_cat(int(category_id)), );// 
set( "segmentation": rle, );// 
set( "score": float(score), );// 
set( } );// 
set( ) );// 
bend();//
end( );//def add_mask( > ----------------------- 1
end();//
event( def dump(self):  );//
branch();//
path();//
path();//  
set( dump_arguments = [ );// 
set( (self.bbox_data, args.bbox_det_file), );// 
set( (self.mask_data, args.mask_det_file), );// 
set( ] );// 
loop( for data, path in dump_arguments: );// 
branch( with open(path, "w") as f: );// 
path();//
set( json.dump(data, f) );// 
bend( );//with open(path, "w") as f:
lend( );//for data, path in dump_arguments:
bend();//
end( );//def dump(self): > ----------------------- 1
end();//
event( def dump_web(self):  );//
branch();//
path();//
path();//  
set(  );// Dumps it in the format for my web app. Warning: bad code ahead!***1
set( config_outs = [ );// 
set( "preserve_aspect_ratio", );// 
set( "use_prediction_module", );// 
set( "use_yolo_regressors", );// 
set( "use_prediction_matching", );// 
set( "train_masks", );// 
set( ] );// 
set( output = { );// 
set( "info": { );// 
set( "Config": {key: getattr(cfg, key) for key in config_outs}, );// 
set( } );// 
set( } );// 
set( image_ids = list(set([x["image_id"] for x in self.bbox_data])) );// 
set( image_ids.sort() );// 
set( image_lookup = {_id: idx for idx, _id in enumerate(image_ids)} );// 
set( output["images"] = [ );// 
set( {"image_id": image_id, "dets": []} for image_id in image_ids );// 
set( ] );// 
set(  );//  These should already be sorted by score with the way prep_metrics works.
loop( for bbox, mask in zip(self.bbox_data, self.mask_data): );// 
set( image_obj = output["images"][image_lookup[bbox["image_id"]]] );// 
set( image_obj["dets"].append( );// 
set( { );// 
set( "score": bbox["score"], );// 
set( "bbox": bbox["bbox"], );// 
set( "category": cfg.dataset.class_names[ );// 
set( get_transformed_cat(bbox["category_id"]) );// 
set( ], );// 
set( "mask": mask["segmentation"], );// 
set( } );// 
set( ) );// 
lend( );//for bbox, mask in zip(self.bbox_data, self.mask_data):
branch( with open(os.path.join(args.web_det_path, "%s.json" % cfg.name), "w") as f: );// 
path();//
set( json.dump(output, f) );// 
bend( );//with open(os.path.join(args.web_det_path, "%s.json" % cfg.name), "w") as f:
bend();//
end( );//def dump_web(self): > ----------------------- 1
bend();//
end( );//class Detections:
end();//

input( def _mask_iou(mask1, mask2, iscrowd=False):  );//
branch();//
path();//
path();// > --------------------------input 0 
branch( with timer.env("Mask IoU"): );// 
path();//
set( ret = mask_iou(mask1, mask2, iscrowd) );// 
bend( );//with timer.env("Mask IoU"):
set( return ret.cpu() );// 
bend();//
end( );//def _mask_iou(mask1, mask2, iscrowd=False): > ----------------------- 0
end();//

input( def _bbox_iou(bbox1, bbox2, iscrowd=False):  );//
branch();//
path();//
path();// > --------------------------input 0 
branch( with timer.env("BBox IoU"): );// 
path();//
set( ret = jaccard(bbox1, bbox2, iscrowd) );// 
bend( );//with timer.env("BBox IoU"):
set( return ret.cpu() );// 
bend();//
end( );//def _bbox_iou(bbox1, bbox2, iscrowd=False): > ----------------------- 0
end();//

input( def prep_metrics(  );//
branch();//
path();//
path();// > --------------------------input 0 
set( ap_data, );// 
set( dets, );// 
set( img, );// 
set( gt, );// 
set( gt_masks, );// 
set( h, );// 
set( w, );// 
set( num_crowd, );// 
set( image_id, );// 
set( detections: Detections = None, );// 
set( ): );// 
set(  );// Returns a list of APs for this image, with each element being for a class***1
branch( if not args.output_coco_json: );// 
path();//
branch( with timer.env("Prepare gt"): );// 
path();//
set( gt_boxes = torch.Tensor(gt[:, :4]) );// 
set( gt_boxes[:, [0, 2]] *= w );// 
set( gt_boxes[:, [1, 3]] *= h );// 
set( gt_classes = list(gt[:, 4].astype(int)) );// 
set( gt_masks = torch.Tensor(gt_masks).view(-1, h * w) );// 
branch( if num_crowd > 0: );// 
path();//
set( split = lambda x: (x[-num_crowd:], x[:-num_crowd]) );// 
set( crowd_boxes, gt_boxes = split(gt_boxes) );// 
set( crowd_masks, gt_masks = split(gt_masks) );// 
set( crowd_classes, gt_classes = split(gt_classes) );// 
bend( );//if num_crowd > 0:
bend( );//with timer.env("Prepare gt"):
bend( );//if not args.output_coco_json:
branch( with timer.env("Postprocess"): );// 
path();//
set( classes, scores, boxes, masks = postprocess( );// 
set( dets, w, h, crop_masks=args.crop, score_threshold=args.score_threshold );// 
set( ) );// 
branch( if classes.size(0) == 0: );// 
path();//
set( return );// 
bend( );//if classes.size(0) == 0:
set( classes = list(classes.cpu().numpy().astype(int)) );// 
branch( if isinstance(scores, list): );// 
path();//
set( box_scores = list(scores[0].cpu().numpy().astype(float)) );// 
set( mask_scores = list(scores[1].cpu().numpy().astype(float)) );// 
path( else: );// 
set( scores = list(scores.cpu().numpy().astype(float)) );// 
set( box_scores = scores );// 
set( mask_scores = scores );// 
bend( );//if isinstance(scores, list):
set( masks = masks.view(-1, h * w).cuda() );// 
set( boxes = boxes.cuda() );// 
bend( );//with timer.env("Postprocess"):
branch( if args.output_coco_json: );// 
path();//
branch( with timer.env("JSON Output"): );// 
path();//
set( boxes = boxes.cpu().numpy() );// 
set( masks = masks.view(-1, h, w).cpu().numpy() );// 
loop( for i in range(masks.shape[0]): );// 
set(  );//  Make sure that the bounding box actually makes sense and a mask was produced
branch( if (boxes[i, 3] - boxes[i, 1]) * (boxes[i, 2] - boxes[i, 0]) > 0: );// 
path();//
set( detections.add_bbox( );// 
set( image_id, classes[i], boxes[i, :], box_scores[i] );// 
set( ) );// 
set( detections.add_mask( );// 
set( image_id, classes[i], masks[i, :, :], mask_scores[i] );// 
set( ) );// 
bend( );//if (boxes[i, 3] - boxes[i, 1]) * (boxes[i, 2] - boxes[i, 0]) > 0:
lend( );//for i in range(masks.shape[0]):
set( return );// 
bend( );//with timer.env("JSON Output"):
bend( );//if args.output_coco_json:
branch( with timer.env("Eval Setup"): );// 
path();//
set( num_pred = len(classes) );// 
set( num_gt = len(gt_classes) );// 
set( mask_iou_cache = _mask_iou(masks, gt_masks) );// 
set( bbox_iou_cache = _bbox_iou(boxes.float(), gt_boxes.float()) );// 
branch( if num_crowd > 0: );// 
path();//
set( crowd_mask_iou_cache = _mask_iou(masks, crowd_masks, iscrowd=True) );// 
set( crowd_bbox_iou_cache = _bbox_iou( );// 
set( boxes.float(), crowd_boxes.float(), iscrowd=True );// 
set( ) );// 
path( else: );// 
set( crowd_mask_iou_cache = None );// 
set( crowd_bbox_iou_cache = None );// 
bend( );//if num_crowd > 0:
set( box_indices = sorted(range(num_pred), key=lambda i: -box_scores[i]) );// 
set( mask_indices = sorted(box_indices, key=lambda i: -mask_scores[i]) );// 
set( iou_types = [ );// 
set( ( );// 
set( "box", );// 
set( lambda i, j: bbox_iou_cache[i, j].item(), );// 
set( lambda i, j: crowd_bbox_iou_cache[i, j].item(), );// 
set( lambda i: box_scores[i], );// 
set( box_indices, );// 
set( ), );// 
set( ( );// 
set( "mask", );// 
set( lambda i, j: mask_iou_cache[i, j].item(), );// 
set( lambda i, j: crowd_mask_iou_cache[i, j].item(), );// 
set( lambda i: mask_scores[i], );// 
set( mask_indices, );// 
set( ), );// 
set( ] );// 
bend( );//with timer.env("Eval Setup"):
set( timer.start("Main loop") );// 
loop( for _class in set(classes + gt_classes): );// 
set( ap_per_iou = [] );// 
set( num_gt_for_class = sum([1 for x in gt_classes if x == _class]) );// 
loop( for iouIdx in range(len(iou_thresholds)): );// 
set( iou_threshold = iou_thresholds[iouIdx] );// 
loop( for iou_type, iou_func, crowd_func, score_func, indices in iou_types: );// 
set( gt_used = [False] * len(gt_classes) );// 
set( ap_obj = ap_data[iou_type][iouIdx][_class] );// 
set( ap_obj.add_gt_positives(num_gt_for_class) );// 
loop( for i in indices: );// 
branch( if classes[i] != _class: );// 
path();//
set( continue );// 
bend( );//if classes[i] != _class:
set( max_iou_found = iou_threshold );// 
set( max_match_idx = -1 );// 
loop( for j in range(num_gt): );// 
branch( if gt_used[j] or gt_classes[j] != _class: );// 
path();//
set( continue );// 
bend( );//if gt_used[j] or gt_classes[j] != _class:
set( iou = iou_func(i, j) );// 
branch( if iou > max_iou_found: );// 
path();//
set( max_iou_found = iou );// 
set( max_match_idx = j );// 
bend( );//if iou > max_iou_found:
lend( );//for j in range(num_gt):
branch( if max_match_idx >= 0: );// 
path();//
set( gt_used[max_match_idx] = True );// 
set( ap_obj.push(score_func(i), True) );// 
path( else: );// 
set(  );//  If the detection matches a crowd, we can just ignore it
set( matched_crowd = False );// 
branch( if num_crowd > 0: );// 
path();//
loop( for j in range(len(crowd_classes)): );// 
branch( if crowd_classes[j] != _class: );// 
path();//
set( continue );// 
bend( );//if crowd_classes[j] != _class:
set( iou = crowd_func(i, j) );// 
branch( if iou > iou_threshold: );// 
path();//
set( matched_crowd = True );// 
set( break );// 
bend( );//if iou > iou_threshold:
lend( );//for j in range(len(crowd_classes)):
bend( );//if num_crowd > 0:
set(  );//  All this crowd code so that we can make sure that our eval code gives the
set(  );//  same result as COCOEval. There aren't even that many crowd annotations to
set(  );//  begin with, but accuracy is of the utmost importance.
branch( if not matched_crowd: );// 
path();//
set( ap_obj.push(score_func(i), False) );// 
bend( );//if not matched_crowd:
bend( );//if max_match_idx >= 0:
lend( );//for i in indices:
lend( );//for iou_type, iou_func, crowd_func, score_func, indices in iou_types:
lend( );//for iouIdx in range(len(iou_thresholds)):
lend( );//for _class in set(classes + gt_classes):
set( timer.stop("Main loop") );// 
bend();//
end( );//def prep_metrics( > ----------------------- 0
end();//;

input( class APDataObject:  );//
branch();//
path();//
path();//  
set( );//BEGIN MULTI LINE COMMENT
set( );//    Stores all the information necessary to calculate the AP for one IoU and one class.
set( );//    Note: I type annotated this because why not.
set( );//END MULTI LINE COMMENT
end();//
event( def __init__(self):  );//
branch();//
path();//
path();//  
set( self.data_points = [] );// 
set( self.num_gt_positives = 0 );// 
bend();//
end( );//def __init__(self): > ----------------------- 1
end();//
event( def push(self, score: float, is_true: bool):  );//
branch();//
path();//
path();//  
set( self.data_points.append((score, is_true)) );// 
bend();//
end( );//def push(self, score: float, is_true: bool): > ----------------------- 1
end();//
event( def add_gt_positives(self, num_positives: int):  );//
branch();//
path();//
path();//  
set(  );// Call this once per image.***1
set( self.num_gt_positives += num_positives );// 
bend();//
end( );//def add_gt_positives(self, num_positives: int): > ----------------------- 1
end();//
event( def is_empty(self) -> bool:  );//
branch();//
path();//
path();//  
set( return len(self.data_points) == 0 and self.num_gt_positives == 0 );// 
bend();//
end( );//def is_empty(self) -> bool: > ----------------------- 1
end();//
event( def get_ap(self) -> float:  );//
branch();//
path();//
path();//  
set(  );// Warning: result not cached.***1
branch( if self.num_gt_positives == 0: );// 
path();//
set( return 0 );// 
bend( );//if self.num_gt_positives == 0:
set(  );//  Sort descending by score
set( self.data_points.sort(key=lambda x: -x[0]) );// 
set( precisions = [] );// 
set( recalls = [] );// 
set( num_true = 0 );// 
set( num_false = 0 );// 
set(  );//  Compute the precision-recall curve. The x axis is recalls and the y axis precisions.
loop( for datum in self.data_points: );// 
set(  );//  datum[1] is whether the detection a true or false positive
branch( if datum[1]: );// 
path();//
set( num_true += 1 );// 
path( else: );// 
set( num_false += 1 );// 
bend( );//if datum[1]:
set( precision = num_true / (num_true + num_false) );// 
set( recall = num_true / self.num_gt_positives );// 
set( precisions.append(precision) );// 
set( recalls.append(recall) );// 
lend( );//for datum in self.data_points:
set(  );//  Smooth the curve by computing [max(precisions[i:]) for i in range(len(precisions))]
set(  );//  Basically, remove any temporary dips from the curve.
set(  );//  At least that's what I think, idk. COCOEval did it so I do too.
loop( for i in range(len(precisions) - 1, 0, -1): );// 
branch( if precisions[i] > precisions[i - 1]: );// 
path();//
set( precisions[i - 1] = precisions[i] );// 
bend( );//if precisions[i] > precisions[i - 1]:
lend( );//for i in range(len(precisions) - 1, 0, -1):
set(  );//  Compute the integral of precision(recall) d_recall from recall=0->1 using fixed-length riemann summation with 101 bars.
set( y_range = [0] * 101   );//  idx 0 is recall == 0.0 and idx 100 is recall == 1.00
set( x_range = np.array([x / 100 for x in range(101)]) );// 
set( recalls = np.array(recalls) );// 
set(  );//  I realize this is weird, but all it does is find the nearest precision(x) for a given x in x_range.
set(  );//  Basically, if the closest recall we have to 0.01 is 0.009 this sets precision(0.01) = precision(0.009).
set(  );//  I approximate the integral this way, because that's how COCOEval does it.
set( indices = np.searchsorted(recalls, x_range, side="left") );// 
loop( for bar_idx, precision_idx in enumerate(indices): );// 
branch( if precision_idx < len(precisions): );// 
path();//
set( y_range[bar_idx] = precisions[precision_idx] );// 
bend( );//if precision_idx < len(precisions):
lend( );//for bar_idx, precision_idx in enumerate(indices):
set(  );//  Finally compute the riemann sum to get our integral.
set(  );//  avg([precision(x) for x in 0:0.01:1])
set( return sum(y_range) / len(y_range) );// 
bend();//
end( );//def get_ap(self) -> float: > ----------------------- 1
bend();//
end( );//class APDataObject:
end();//

input( def badhash(x):  );//
branch();//
path();//
path();// > --------------------------input 0 
set( );//BEGIN MULTI LINE COMMENT
set( );//    Just a quick and dirty hash function for doing a deterministic shuffle based on image_id.
set( );//
set( );//    Source:
set( );//    https://stackoverflow.com/questions/664014/what-integer-hash-function-are-good-that-accepts-an-integer-hash-key
set( );//END MULTI LINE COMMENT
set( x = (((x >> 16) ^ x) * 0x045D9F3B) & 0xFFFFFFFF );// 
set( x = (((x >> 16) ^ x) * 0x045D9F3B) & 0xFFFFFFFF );// 
set( x = ((x >> 16) ^ x) & 0xFFFFFFFF );// 
set( return x );// 
bend();//
end( );//def badhash(x): > ----------------------- 0
end();//

input( def evalimage(net: Yolact, path: str, save_path: str = None):  );//
branch();//
path();//
path();// > --------------------------input 0 
set( frame = torch.from_numpy(cv2.imread(path)).cuda().float() );// 
set( batch = FastBaseTransform()(frame.unsqueeze(0)) );// 
set( preds = net(batch) );// 
set( img_numpy = prep_display(preds, frame, None, None, undo_transform=False) );// 
branch( if save_path is None: );// 
path();//
set( img_numpy = img_numpy[:, :, (2, 1, 0)] );// 
bend( );//if save_path is None:
branch( if save_path is None: );// 
path();//
set( plt.imshow(img_numpy) );// 
set( plt.title(path) );// 
set( plt.show() );// 
path( else: );// 
set( cv2.imwrite(save_path, img_numpy) );// 
bend( );//if save_path is None:
bend();//
end( );//def evalimage(net: Yolact, path: str, save_path: str = None): > ----------------------- 0
end();//

input( def evalimages(net: Yolact, input_folder: str, output_folder: str):  );//
branch();//
path();//
path();// > --------------------------input 0 
branch( if not os.path.exists(output_folder): );// 
path();//
set( os.mkdir(output_folder) );// 
bend( );//if not os.path.exists(output_folder):
set( print() );// 
loop( for p in Path(input_folder).glob("*"): );// 
set( path = str(p) );// 
set( name = os.path.basename(path) );// 
set( name = ".".join(name.split(".")[:-1]) + ".png" );//
set( out_path = os.path.join(output_folder, name) );// 
set( evalimage(net, path, out_path) );// 
set( print(path + " -> " + out_path) );// 
lend( );//for p in Path(input_folder).glob("*"):
set( print("Done.") );// 
bend();//
end( );//def evalimages(net: Yolact, input_folder: str, output_folder: str): > ----------------------- 0
event( from multiprocessing.pool import ThreadPool );// 
event( from queue import Queue );// 
end();//;

input( class CustomDataParallel(torch.nn.DataParallel):  );//
branch();//
path();//
path();//  
set(  );// A Custom Data Parallel class that properly gathers lists of dictionaries.***0
end();//
event( def gather(self, outputs, output_device):  );//
branch();//
path();//
path();//  
set(  );//  Note that I don't actually want to convert everything to the output_device
set( return sum(outputs, []) );// 
bend();//
end( );//def gather(self, outputs, output_device): > ----------------------- 1
bend();//
end( );//class CustomDataParallel(torch.nn.DataParallel):
end();//

input( def evalvideo(net: Yolact, path: str, out_path: str = None):  );//
branch();//
path();//
path();// > --------------------------input 0 
set(  );//  If the path is a digit, parse it as a webcam index
set( is_webcam = path.isdigit() );// 
set(  );//  If the input image size is constant, this make things faster (hence why we can use it in a video setting).
set( cudnn.benchmark = True );// 
branch( if is_webcam: );// 
path();//
set( vid = cv2.VideoCapture(int(path)) );// 
path( else: );// 
set( vid = cv2.VideoCapture(path) );// 
bend( );//if is_webcam:
branch( if not vid.isOpened(): );// 
path();//
set( print('Could not open video "%s"' % path) );// 
set( exit(-1) );// 
bend( );//if not vid.isOpened():
set( target_fps = round(vid.get(cv2.CAP_PROP_FPS)) );// 
set( frame_width = round(vid.get(cv2.CAP_PROP_FRAME_WIDTH)) );// 
set( frame_height = round(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)) );// 
branch( if is_webcam: );// 
path();//
set( num_frames = float("inf") );// 
path( else: );// 
set( num_frames = round(vid.get(cv2.CAP_PROP_FRAME_COUNT)) );// 
bend( );//if is_webcam:
set( net = CustomDataParallel(net).cuda() );// 
set( transform = torch.nn.DataParallel(FastBaseTransform()).cuda() );// 
set( frame_times = MovingAverage(100) );// 
set( fps = 0 );// 
set( frame_time_target = 1 / target_fps );// 
set( running = True );// 
set( fps_str = "" );//
set( vid_done = False );// 
set( frames_displayed = 0 );// 
branch( if out_path is not None: );// 
path();//
set( out = cv2.VideoWriter( );// 
set( out_path, );// 
set( cv2.VideoWriter_fourcc(*"mp4v"), );// 
set( target_fps, );// 
set( (frame_width, frame_height), );// 
set( ) );// 
bend( );//if out_path is not None:
end();//
event( def cleanup_and_exit():  );//
branch();//
path();//
path();//  
set( print() );// 
set( pool.terminate() );// 
set( vid.release() );// 
branch( if out_path is not None: );// 
path();//
set( out.release() );// 
bend( );//if out_path is not None:
set( cv2.destroyAllWindows() );// 
set( exit() );// 
bend();//
end( );//def cleanup_and_exit(): > ----------------------- 1
end();//
event( def get_next_frame(vid):  );//
branch();//
path();//
path();//  
set( frames = [] );// 
loop( for idx in range(args.video_multiframe): );// 
set( frame = vid.read()[1] );// 
branch( if frame is None: );// 
path();//
set( return frames );// 
bend( );//if frame is None:
set( frames.append(frame) );// 
lend( );//for idx in range(args.video_multiframe):
set( return frames );// 
bend();//
end( );//def get_next_frame(vid): > ----------------------- 1
end();//
event( def transform_frame(frames):  );//
branch();//
path();//
path();//  
branch( with torch.no_grad(): );// 
path();//
set( frames = [torch.from_numpy(frame).cuda().float() for frame in frames] );// 
set( return frames, transform(torch.stack(frames, 0)) );// 
bend( );//with torch.no_grad():
bend();//
end( );//def transform_frame(frames): > ----------------------- 1
end();//
event( def eval_network(inp):  );//
branch();//
path();//
path();//  
branch( with torch.no_grad(): );// 
path();//
set( frames, imgs = inp );// 
set( num_extra = 0 );// 
loop( while imgs.size(0) < args.video_multiframe: );// 
set( imgs = torch.cat([imgs, imgs[0].unsqueeze(0)], dim=0) );// 
set( num_extra += 1 );// 
lend( );//while imgs.size(0) < args.video_multiframe:
set( out = net(imgs) );// 
branch( if num_extra > 0: );// 
path();//
set( out = out[:-num_extra] );// 
bend( );//if num_extra > 0:
set( return frames, out );// 
bend( );//with torch.no_grad():
bend();//
end( );//def eval_network(inp): > ----------------------- 1
end();//
event( def prep_frame(inp, fps_str):  );//
branch();//
path();//
path();//  
branch( with torch.no_grad(): );// 
path();//
set( frame, preds = inp );// 
set( return prep_display( );// 
set( preds, );// 
set( frame, );// 
set( None, );// 
set( None, );// 
set( undo_transform=False, );// 
set( class_color=True, );// 
set( fps_str=fps_str, );// 
set( ) );// 
bend( );//with torch.no_grad():
bend();//
end( );//def prep_frame(inp, fps_str): > ----------------------- 1
set( frame_buffer = Queue() );// 
set( video_fps = 0 );// 
set(  );//  All this timing code to make sure that
end();//
event( def play_video():  );//
branch();//
path();//
path();//  
branch( try: );// 
path();//
set( nonlocal frame_buffer, running, video_fps, is_webcam, num_frames, frames_displayed, vid_done );// 
set( video_frame_times = MovingAverage(100) );// 
set( frame_time_stabilizer = frame_time_target );// 
set( last_time = None );// 
set( stabilizer_step = 0.0005 );// 
set( progress_bar = ProgressBar(30, num_frames) );// 
loop( while running: );// 
set( frame_time_start = time.time() );// 
branch( if not frame_buffer.empty(): );// 
path();//
set( next_time = time.time() );// 
branch( if last_time is not None: );// 
path();//
set( video_frame_times.add(next_time - last_time) );// 
set( video_fps = 1 / video_frame_times.get_avg() );// 
bend( );//if last_time is not None:
branch( if out_path is None: );// 
path();//
set( cv2.imshow(path, frame_buffer.get()) );// 
path( else: );// 
set( out.write(frame_buffer.get()) );// 
bend( );//if out_path is None:
set( frames_displayed += 1 );// 
set( last_time = next_time );// 
branch( if out_path is not None: );// 
path();//
branch( if video_frame_times.get_avg() == 0: );// 
path();//
set( fps = 0 );// 
path( else: );// 
set( fps = 1 / video_frame_times.get_avg() );// 
bend( );//if video_frame_times.get_avg() == 0:
set( progress = frames_displayed / num_frames * 100 );// 
set( progress_bar.set_val(frames_displayed) );// 
set( print( );// 
set( "\rProcessing Frames  %s %6d / %6d (%5.2f%%)%5.2f fps" );//
set( % ( );// 
set( repr(progress_bar), );// 
set( frames_displayed, );// 
set( num_frames, );// 
set( progress, );// 
set( fps, );// 
set( ), );// 
set( end="", );// 
set( ) );// 
bend( );//if out_path is not None:
bend( );//if not frame_buffer.empty():
set(  );// 197)
branch( if out_path is None and cv2.waitKey(1) == 27: );// 
path();//
set(  );//  Press Escape to close
set( running = False );// 
bend( );//if out_path is None and cv2.waitKey(1) == 27:
branch( if not (frames_displayed < num_frames): );// 
path();//
set( running = False );// 
bend( );//if not (frames_displayed < num_frames):
branch( if not vid_done: );// 
path();//
set( buffer_size = frame_buffer.qsize() );// 
branch( if buffer_size < args.video_multiframe: );// 
path();//
set( frame_time_stabilizer += stabilizer_step );// 
path( elif buffer_size > args.video_multiframe: );// 
set( frame_time_stabilizer -= stabilizer_step );// 
branch( if frame_time_stabilizer < 0: );// 
path();//
set( frame_time_stabilizer = 0 );// 
bend( );//if frame_time_stabilizer < 0:
bend( );//if buffer_size < args.video_multiframe:
set( new_target = ( );// 
set( frame_time_stabilizer );// 
set( if is_webcam );// 
set( else max(frame_time_stabilizer, frame_time_target) );// 
set( ) );// 
path( else: );// 
set( new_target = frame_time_target );// 
bend( );//if not vid_done:
set( next_frame_target = max(2 * new_target - video_frame_times.get_avg(), 0) );// 
set( target_time = ( );// 
set( frame_time_start + next_frame_target - 0.001 );// 
set( )   );//  Let's just subtract a millisecond to be safe
branch( if out_path is None or args.emulate_playback: );// 
path();//
set(  );//  This gives more accurate timing than if sleeping the whole amount at once
loop( while time.time() < target_time: );// 
set( time.sleep(0.001) );// 
lend( );//while time.time() < target_time:
path( else: );// 
set(  );//  Let's not starve the main thread, now
set( time.sleep(0.001) );// 
bend( );//if out_path is None or args.emulate_playback:
lend( );//while running:
path( except: );// 
set(  );// 197 for why this is necessary
event( import traceback );// 
set( traceback.print_exc() );// 
bend( );//try:
bend();//
end( );//def play_video(): > ----------------------- 1
set( extract_frame = lambda x, i: ( );// 
set( x[0][i] );// 
set( if x[1][i]["detection"] is None );// 
set( else x[0][i].to(x[1][i]["detection"]["box"].device), );// 
set( [x[1][i]], );// 
set( ) );// 
set(  );//  Prime the network on the first frame because I do some thread unsafe things otherwise
set( print("Initializing model... ", end="") );// 
set( first_batch = eval_network(transform_frame(get_next_frame(vid))) );// 
set( print("Done.") );// 
set(  );//  For each frame the sequence of functions it needs to go through to be processed (in reversed order)
set( sequence = [prep_frame, eval_network, transform_frame] );// 
set( pool = ThreadPool(processes=len(sequence) + args.video_multiframe + 2) );// 
set( pool.apply_async(play_video) );// 
set( active_frames = [ );// 
set( {"value": extract_frame(first_batch, i), "idx": 0} );// 
set( for i in range(len(first_batch[0])) );// 
set( ] );// 
set( print() );// 
branch( if out_path is None: );// 
path();//
set( print("Press Escape to close.") );// 
bend( );//if out_path is None:
branch( try: );// 
path();//
loop( while vid.isOpened() and running: );// 
set(  );//  Hard limit on frames in buffer so we don't run out of memory >.>
loop( while frame_buffer.qsize() > 100: );// 
set( time.sleep(0.001) );// 
lend( );//while frame_buffer.qsize() > 100:
set( start_time = time.time() );// 
set(  );//  Start loading the next frames from the disk
branch( if not vid_done: );// 
path();//
set( next_frames = pool.apply_async(get_next_frame, args=(vid,)) );// 
path( else: );// 
set( next_frames = None );// 
bend( );//if not vid_done:
branch( if not (vid_done and len(active_frames) == 0): );// 
path();//
set(  );//  For each frame in our active processing queue, dispatch a job
set(  );//  for that frame using the current function in the sequence
loop( for frame in active_frames: );// 
set( _args = [frame["value"]] );// 
branch( if frame["idx"] == 0: );// 
path();//
set( _args.append(fps_str) );// 
bend( );//if frame["idx"] == 0:
set( frame["value"] = pool.apply_async( );// 
set( sequence[frame["idx"]], args=_args );// 
set( ) );// 
lend( );//for frame in active_frames:
set(  );//  For each frame whose job was the last in the sequence (i.e. for all final outputs)
loop( for frame in active_frames: );// 
branch( if frame["idx"] == 0: );// 
path();//
set( frame_buffer.put(frame["value"].get()) );// 
bend( );//if frame["idx"] == 0:
lend( );//for frame in active_frames:
set(  );//  Remove the finished frames from the processing queue
set( active_frames = [x for x in active_frames if x["idx"] > 0] );// 
set(  );//  Finish evaluating every frame in the processing queue and advanced their position in the sequence
loop( for frame in list(reversed(active_frames)): );// 
set( frame["value"] = frame["value"].get() );// 
set( frame["idx"] -= 1 );// 
branch( if frame["idx"] == 0: );// 
path();//
set(  );//  Split this up into individual threads for prep_frame since it doesn't support batch size
set( active_frames += [ );// 
set( {"value": extract_frame(frame["value"], i), "idx": 0} );// 
set( for i in range(1, len(frame["value"][0])) );// 
set( ] );// 
set( frame["value"] = extract_frame(frame["value"], 0) );// 
bend( );//if frame["idx"] == 0:
lend( );//for frame in list(reversed(active_frames)):
set(  );//  Finish loading in the next frames and add them to the processing queue
branch( if next_frames is not None: );// 
path();//
set( frames = next_frames.get() );// 
branch( if len(frames) == 0: );// 
path();//
set( vid_done = True );// 
path( else: );// 
set( active_frames.append( );// 
set( {"value": frames, "idx": len(sequence) - 1} );// 
set( ) );// 
bend( );//if len(frames) == 0:
bend( );//if next_frames is not None:
set(  );//  Compute FPS
set( frame_times.add(time.time() - start_time) );// 
set( fps = args.video_multiframe / frame_times.get_avg() );// 
path( else: );// 
set( fps = 0 );// 
bend( );//if not (vid_done and len(active_frames) == 0):
set( fps_str = ( );// 
set( "Processing FPS: %.2f | Video Playback FPS: %.2f | Frames in Buffer: %d" );//
set( % (fps, video_fps, frame_buffer.qsize()) );// 
set( ) );// 
branch( if not args.display_fps: );// 
path();//
set( print("\r" + fps_str + "", end="") );// 
bend( );//if not args.display_fps:
lend( );//while vid.isOpened() and running:
path( except KeyboardInterrupt: );// 
set( print("\nStopping...") );// 
bend( );//try:
set( cleanup_and_exit() );// 
bend();//
end( );//def evalvideo(net: Yolact, path: str, out_path: str = None): > ----------------------- 0
end();//

input( def evaluate(net: Yolact, dataset, train_mode=False):  );//
branch();//
path();//
path();// > --------------------------input 0 
set( net.detect.use_fast_nms = args.fast_nms );// 
set( net.detect.use_cross_class_nms = args.cross_class_nms );// 
set( cfg.mask_proto_debug = args.mask_proto_debug );// 
set(  );//  TODO Currently we do not support Fast Mask Re-scroing in evalimage, evalimages, and evalvideo
branch( if args.image is not None: );// 
path();//
branch( if ":" in args.image: );// 
path();//
set( inp, out = args.image.split(":") );// 
set( evalimage(net, inp, out) );// 
path( else: );// 
set( evalimage(net, args.image) );// 
bend( );//if ":" in args.image:
set( return );// 
path( elif args.images is not None: );// 
set( inp, out = args.images.split(":") );// 
set( evalimages(net, inp, out) );// 
set( return );// 
path( elif args.video is not None: );// 
branch( if ":" in args.video: );// 
path();//
set( inp, out = args.video.split(":") );// 
set( evalvideo(net, inp, out) );// 
path( else: );// 
set( evalvideo(net, args.video) );// 
bend( );//if ":" in args.video:
set( return );// 
bend( );//if args.image is not None:
set( frame_times = MovingAverage() );// 
set( dataset_size = ( );// 
set( len(dataset) if args.max_images < 0 else min(args.max_images, len(dataset)) );// 
set( ) );// 
set( progress_bar = ProgressBar(30, dataset_size) );// 
set( print() );// 
branch( if not args.display and not args.benchmark: );// 
path();//
set(  );//  For each class and iou, stores tuples (score, isPositive)
set(  );//  Index ap_data[type][iouIdx][classIdx]
set( ap_data = { );// 
set( "box": [ );// 
set( [APDataObject() for _ in cfg.dataset.class_names] );// 
set( for _ in iou_thresholds );// 
set( ], );// 
set( "mask": [ );// 
set( [APDataObject() for _ in cfg.dataset.class_names] );// 
set( for _ in iou_thresholds );// 
set( ], );// 
set( } );// 
set( detections = Detections() );// 
path( else: );// 
set( timer.disable("Load Data") );// 
bend( );//if not args.display and not args.benchmark:
set( dataset_indices = list(range(len(dataset))) );// 
branch( if args.shuffle: );// 
path();//
set( random.shuffle(dataset_indices) );// 
path( elif not args.no_sort: );// 
set(  );//  Do a deterministic shuffle based on the image ids
set(  );// 
set(  );//  I do this because on python 3.5 dictionary key order is *random*, while in 3.6 it's
set(  );//  the order of insertion. That means on python 3.6, the images come in the order they are in
set(  );//  in the annotations file. For some reason, the first images in the annotations file are
set(  );//  the hardest. To combat this, I use a hard-coded hash function based on the image ids
set(  );//  to shuffle the indices we use. That way, no matter what python version or how pycocotools
set(  );//  handles the data, we get the same result every time.
set( hashed = [badhash(x) for x in dataset.ids] );// 
set( dataset_indices.sort(key=lambda x: hashed[x]) );// 
bend( );//if args.shuffle:
set( dataset_indices = dataset_indices[:dataset_size] );// 
branch( try: );// 
path();//
set(  );//  Main eval loop
loop( for it, image_idx in enumerate(dataset_indices): );// 
set( timer.reset() );// 
branch( with timer.env("Load Data"): );// 
path();//
set( img, gt, gt_masks, h, w, num_crowd = dataset.pull_item(image_idx) );// 
set(  );//  Test flag, do not upvote
branch( if cfg.mask_proto_debug: );// 
path();//
branch( with open("scripts/info.txt", "w") as f: );// 
path();//
set( f.write(str(dataset.ids[image_idx])) );// 
bend( );//with open("scripts/info.txt", "w") as f:
set( np.save("scripts/gt.npy", gt_masks) );// 
bend( );//if cfg.mask_proto_debug:
set( batch = Variable(img.unsqueeze(0)) );// 
branch( if args.cuda: );// 
path();//
set( batch = batch.cuda() );// 
bend( );//if args.cuda:
bend( );//with timer.env("Load Data"):
branch( with timer.env("Network Extra"): );// 
path();//
set( preds = net(batch) );// 
bend( );//with timer.env("Network Extra"):
set(  );//  Perform the meat of the operation here depending on our mode.
branch( if args.display: );// 
path();//
set( img_numpy = prep_display(preds, img, h, w) );// 
path( elif args.benchmark: );// 
set( prep_benchmark(preds, h, w) );// 
path( else: );// 
set( prep_metrics( );// 
set( ap_data, );// 
set( preds, );// 
set( img, );// 
set( gt, );// 
set( gt_masks, );// 
set( h, );// 
set( w, );// 
set( num_crowd, );// 
set( dataset.ids[image_idx], );// 
set( detections, );// 
set( ) );// 
bend( );//if args.display:
set(  );//  First couple of images take longer because we're constructing the graph.
set(  );//  Since that's technically initialization, don't include those in the FPS calculations.
branch( if it > 1: );// 
path();//
set( frame_times.add(timer.total_time()) );// 
bend( );//if it > 1:
branch( if args.display: );// 
path();//
branch( if it > 1: );// 
path();//
output( print("Avg FPS: %.4f" % (1 / frame_times.get_avg())) );// 
bend( );//if it > 1:
set( plt.imshow(img_numpy) );// 
set( plt.title(str(dataset.ids[image_idx])) );// 
set( plt.show() );// 
path( elif not args.no_bar: );// 
branch( if it > 1: );// 
path();//
set( fps = 1 / frame_times.get_avg() );// 
path( else: );// 
set( fps = 0 );// 
bend( );//if it > 1:
set( progress = (it + 1) / dataset_size * 100 );// 
set( progress_bar.set_val(it + 1) );// 
set( print( );// 
set( "\rProcessing Images  %s %6d / %6d (%5.2f%%)%5.2f fps" );//
set( % (repr(progress_bar), it + 1, dataset_size, progress, fps), );// 
set( end="", );// 
set( ) );// 
bend( );//if args.display:
lend( );//for it, image_idx in enumerate(dataset_indices):
branch( if not args.display and not args.benchmark: );// 
path();//
set( print() );// 
branch( if args.output_coco_json: );// 
path();//
set( print("Dumping detections...") );// 
branch( if args.output_web_json: );// 
path();//
set( detections.dump_web() );// 
path( else: );// 
set( detections.dump() );// 
bend( );//if args.output_web_json:
path( else: );// 
branch( if not train_mode: );// 
path();//
set( print("Saving data...") );// 
branch( with open(args.ap_data_file, "wb") as f: );// 
path();//
set( pickle.dump(ap_data, f) );// 
bend( );//with open(args.ap_data_file, "wb") as f:
bend( );//if not train_mode:
set( return calc_map(ap_data) );// 
bend( );//if args.output_coco_json:
path( elif args.benchmark: );// 
set( print() );// 
set( print() );// 
output( print("Stats for the last frame:") );// 
set( timer.print_stats() );// 
set( avg_seconds = frame_times.get_avg() );// 
set( print( );// 
set( "Average: %5.2f fps, %5.2f ms" );//
set( % (1 / frame_times.get_avg(), 1000 * avg_seconds) );// 
set( ) );// 
bend( );//if not args.display and not args.benchmark:
path( except KeyboardInterrupt: );// 
set( print("Stopping...") );// 
bend( );//try:
bend();//
end( );//def evaluate(net: Yolact, dataset, train_mode=False): > ----------------------- 0
end();//

input( def calc_map(ap_data):  );//
branch();//
path();//
path();// > --------------------------input 0 
set( print("Calculating mAP...") );// 
set( aps = [{"box": [], "mask": []} for _ in iou_thresholds] );// 
loop( for _class in range(len(cfg.dataset.class_names)): );// 
loop( for iou_idx in range(len(iou_thresholds)): );// 
loop( for iou_type in ("box", "mask"): );// 
set( ap_obj = ap_data[iou_type][iou_idx][_class] );// 
branch( if not ap_obj.is_empty(): );// 
path();//
set( aps[iou_idx][iou_type].append(ap_obj.get_ap()) );// 
bend( );//if not ap_obj.is_empty():
lend( );//for iou_type in ("box", "mask"):
lend( );//for iou_idx in range(len(iou_thresholds)):
lend( );//for _class in range(len(cfg.dataset.class_names)):
set( all_maps = {"box": OrderedDict(), "mask": OrderedDict()} );// 
set(  );//  Looking back at it, this code is really hard to read :/
loop( for iou_type in ("box", "mask"): );// 
set( all_maps[iou_type]["all"] = 0   );//  Make this first in the ordereddict
loop( for i, threshold in enumerate(iou_thresholds): );// 
set( mAP = ( );// 
set( sum(aps[i][iou_type]) / len(aps[i][iou_type]) * 100 );// 
branch( if len(aps[i][iou_type]) > 0 );// 
path();//
bend( );//if len(aps[i][iou_type]) > 0
set( else 0 );// 
set( ) );// 
set( all_maps[iou_type][int(threshold * 100)] = mAP );// 
lend( );//for i, threshold in enumerate(iou_thresholds):
set( all_maps[iou_type]["all"] = sum(all_maps[iou_type].values()) / ( );// 
set( len(all_maps[iou_type].values()) - 1 );// 
set( ) );// 
lend( );//for iou_type in ("box", "mask"):
set( print_maps(all_maps) );// 
set(  );//  Put in a prettier format so we can serialize it to json during training
set( all_maps = {k: {j: round(u, 2) for j, u in v.items()} for k, v in all_maps.items()} );// 
set( return all_maps );// 
bend();//
end( );//def calc_map(ap_data): > ----------------------- 0
end();//

input( def print_maps(all_maps):  );//
branch();//
path();//
path();// > --------------------------input 0 
set(  );//  Warning: hacky
set( make_row = lambda vals: (" %5s |" * len(vals)) % tuple(vals) );// 
set( make_sep = lambda n: ("-------+" * n) );// 
set( print() );// 
set( print( );// 
set( make_row( );// 
set( [""] );// 
set( + [ );// 
set( (".%d " % x if isinstance(x, int) else x + " ") );// 
set( for x in all_maps["box"].keys() );// 
set( ] );// 
set( ) );// 
set( ) );// 
set( print(make_sep(len(all_maps["box"]) + 1)) );// 
loop( for iou_type in ("box", "mask"): );// 
set( print( );// 
set( make_row( );// 
set( [iou_type] );// 
set( + [ );// 
set( "%.2f" % x if x < 100 else "%.1f" % x );// 
set( for x in all_maps[iou_type].values() );// 
set( ] );// 
set( ) );// 
set( ) );// 
lend( );//for iou_type in ("box", "mask"):
set( print(make_sep(len(all_maps["box"]) + 1)) );// 
set( print() );// 
bend();//
end( );//def print_maps(all_maps): > ----------------------- 0
branch( if __name__ == "__main__": );// 
path();//
set( parse_args() );// 
branch( if args.config is not None: );// 
path();//
set( set_cfg(args.config) );// 
bend( );//if args.config is not None:
branch( if args.trained_model == "interrupt": );// 
path();//
set( args.trained_model = SavePath.get_interrupt("weights/") );// 
path( elif args.trained_model == "latest": );// 
set( args.trained_model = SavePath.get_latest("weights/", cfg.name) );// 
bend( );//if args.trained_model == "interrupt":
branch( if args.config is None: );// 
path();//
set( model_path = SavePath.from_str(args.trained_model) );// 
set(  );//  TODO: Bad practice? Probably want to do a name lookup instead.
set( args.config = model_path.model_name + "_config" );//
set( print("Config not specified. Parsed %s from the file name.\n" % args.config) );// 
set( set_cfg(args.config) );// 
bend( );//if args.config is None:
branch( if args.detect: );// 
path();//
set( cfg.eval_mask_branch = False );// 
bend( );//if args.detect:
branch( if args.dataset is not None: );// 
path();//
set( set_dataset(args.dataset) );// 
bend( );//if args.dataset is not None:
branch( with torch.no_grad(): );// 
path();//
branch( if not os.path.exists("results"): );// 
path();//
set( os.makedirs("results") );// 
bend( );//if not os.path.exists("results"):
branch( if args.cuda: );// 
path();//
set( cudnn.fastest = True );// 
set( torch.set_default_tensor_type("torch.cuda.FloatTensor") );// 
path( else: );// 
set( torch.set_default_tensor_type("torch.FloatTensor") );// 
bend( );//if args.cuda:
branch( if args.resume and not args.display: );// 
path();//
branch( with open(args.ap_data_file, "rb") as f: );// 
path();//
set( ap_data = pickle.load(f) );// 
bend( );//with open(args.ap_data_file, "rb") as f:
set( calc_map(ap_data) );// 
set( exit() );// 
bend( );//if args.resume and not args.display:
branch( if args.image is None and args.video is None and args.images is None: );// 
path();//
set( dataset = COCODetection( );// 
set( cfg.dataset.valid_images, );// 
set( cfg.dataset.valid_info, );// 
set( transform=BaseTransform(), );// 
set( has_gt=cfg.dataset.has_gt, );// 
set( ) );// 
set( prep_coco_cats() );// 
path( else: );// 
set( dataset = None );// 
bend( );//if args.image is None and args.video is None and args.images is None:
set( print("Loading model...", end="") );// 
set( net = Yolact() );// 
set( net.load_weights(args.trained_model) );// 
set( net.eval() );// 
set( print(" Done.") );// 
branch( if args.cuda: );// 
path();//
set( net = net.cuda() );// 
bend( );//if args.cuda:
set( evaluate(net, dataset) );// 
bend( );//with torch.no_grad():
bend( );//if __name__ == "__main__":



;INSECTA EMBEDDED SESSION INFORMATION
; 255 16777215 65280 16777088 16711680 13158600 13158600 0 255 255 9895835 6946660 3289650
;    eval.py_   #   .
; notepad.exe
;INSECTA EMBEDDED ALTSESSION INFORMATION
; 1852 129 1828 1737 1 100   320   60    python.key  0